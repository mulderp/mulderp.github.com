---
title: RIP Marvin Minsky
layout: post
tags: ai
---
It is sad news to heard that Marvin Minsky passed away. He was a revolting mind who challenged our current views on computers and psychology. While programming is mainly about solving puzzles, programming is also about learning and making discoveries. This was one of the lessons prof. Minsky taught me. And, it was one of the original ideas behind my blog "thinkingonthinking" in the first place.

In honour to prof. Minsky, I want to share some email exchanges we had:

## 2008

> Marvin Minsky <minsky@media.mit.edu>
> 11/26/08 to me 
On Nov 12, 2008, at 3:23 PM, Mulder Patrick wrote:

Hi Marvin,

I just discovered your writings at wiki.laptop.org on how education
could put incentives on discoveries for children.

> Yes.  I'm right now trying to finish Memo 5, which is about whether we can *explicitly* teach children new ways to think, etc.
> I think I should copy those memos to my web page.


Also, I recently learned about values within "teams" and how they
prevent on developing individual views:

> Interesting issue: does teaching "cooperation" conflict with learning to develop one's own ideas?


http://www.ted.com/index.php/talks/jonathan_haidt_on_the_moral_mind.html
http://www.yourmorals.org/explore.php

I couldn't quickly extract a clear idea from these. Does Jonathan Haidt have a good paper on this?


> Other questions:
> *It is not possible to regularly order your first book on computers.
> Are there plans to re-publish the book? Do you have it as PDF or text?

Yes, I'll try to republish it.  I encountered resistance, the last time I tried, because publishers insisted on something new.  But I wrote it so that it would never be outdated!

> *I have your book on Perceptrons. Does topology play a role in modern
> computers?  How can it teach us to make better abstractions?

Well, to begin with, just deciding if there is more than one object in a visual scene need a topological process.  Or counting.  Or X is inside Y, etc.  Topology abstracts relationships that don't depend on specific shapes.    In any case, I suspect that it would be a better subject for children than arithmetic is.  I have an age 10 granddaughter whose hobby is abstract category theory; I think she's likely to become a powerful mathematician.

What's interesting about that Perceptrons book is that, so far as I can see, no current neural-network practitioners understand its ideas at all!  So far as I can see, all of its conclusions still apply to multilayer networks.    

## 2006

minsky@media.mit.edu schrieb:
>   The power of consciousness comes not
> from ceaseless change of state, but from having enough stability to
> discern significant changes in your surroundings.  To "notice" change
> requires the ability to resist it, in order to sense what persists
> through time, but one can do this only by being able to examine and
> compare descriptions from the recent past.  We notice change in spite
> of change, and not because of it.


## 2005 (1)

>Dear Prof. Minsky,
>
>lately I was thinking that computing systems never can
>be "intelligent" (I haven't thought about a definition
>of intelligence yet, but I mean the function of
>solving problems autonomely). As far as I understand
>VLSI systems, they are all finite state machines.


Brain are finite-state machines, too.  So far as present-day physics can say.

## 2005 (2)

>I quite liked the last answer I got from you
>(10/02/2003), that brains could be seen as
>finite-state machines. I was wondering lately what the
>evolutionary reason was, that human has a certain
>number of neuron cells.
>
>I am working on integrating more transistors in
>silicon, and at the moment we have around 400 milion
>transistors in an Intel pentium chip (90nm gate
>length). Moore's Law is predicting several billions
>transistors in some years. At the moment the biggest
>bottleneck is seen in the power consumption (=heat
>production). Do you know if there is a similar problem
>for the human brain?

That is a very interesting question! In fact, many evolutionary 
biologists  think in terms of power consumption,  and regard the 
brain as so power-hungry that this is an important factor in natural 
selection;  they conclude that the reason why so few animals are 
intelligent,  is that it doesn't have very much cost-benefit!

  Here is an amusing fact:  a typical human being consumes about 130 
  watts of power.  All this power comes from pumping protons through 
  the membranes of the mitochondria,  against a potential of about 300 
  millivolts.  Therefore we are powered by a current of about 500 
  amperes!

  >I am still busy in reading some of your work from your
  >homepage... I think there will many question popup.
  >Just one last remark, I have read your article about
  >music and meaning. I found there were extremely
  >interesting thoughts in it.

  Thank you.    Many composers have told me that, so far as they know, 
  that is the only good article specifically about theories of why 
  people like music.    Isn't that strange, considering that many 
  people spend such a large fraction of their lives (and incomes) in it?

  >ps: I have recently read the book of Bertrand Russel
  >"Problems of Philosophy". Do you have suggestions of
  >other books where perception of things/reality is
  >examined ? Thank you very much for your help.

  Yes, very few philosophers have as much common sense as Russell did. 
  The only ones I respect today are  Aaron Sloman,  Daniel Dennett, 
  and John McCarthy.  ( Sloman and McCarthy have good Web pages.)  It 
  is peculiar how few modern thinkers look so deeply into questions 
  about how the mind really works.  But I still find important ideas 
  in much older psychology books, especially Sigmund Freud, William 
  James, Francis Galton,  Wilhelm Wundt, and some yet more ancient 
  works by Aristotle and Augustine.    Needless to say, very few 
  contemporary students or researchers have read any of these!

